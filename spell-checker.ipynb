{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Useful globals\n",
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find the data\n",
    "base_dir = \"./data\"\n",
    "\n",
    "def load_data(language):\n",
    "    \"\"\"\n",
    "        Loads the books to be processed\n",
    "        and returns them as a list of strings.\n",
    "\n",
    "        Input:\n",
    "            string: Language whose books to load\n",
    "            number: Number of books to process\n",
    "\n",
    "        Ouput:\n",
    "            list: List of strings of the loaded books\n",
    "    \"\"\"\n",
    "    # Output\n",
    "    books = []\n",
    "\n",
    "    # Working path\n",
    "    wrk_path = \"{}/{}\".format(base_dir, language)\n",
    "\n",
    "    # Books' filenames\n",
    "    book_files = os.listdir(wrk_path)\n",
    "\n",
    "    # Read each book and append\n",
    "    for book_file in book_files:\n",
    "        input_file = \"{}/{}\".format(wrk_path, book_file)\n",
    "        with open(input_file) as f:\n",
    "            book = f.read()\n",
    "            books.append(book)\n",
    "\n",
    "    # Useful logs about books\n",
    "    print(\"Loaded {} books for {}.\".format(len(books), language))\n",
    "    for i in range(len(books)):\n",
    "        print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 books for english.\n",
      "There are 74964 words in book2.txt.\n",
      "There are 26436 words in book3.txt.\n",
      "There are 121561 words in book1.txt.\n",
      "There are 211862 words in book4.txt.\n",
      "There are 135594 words in book5.txt.\n",
      "There are 37901 words in book7.txt.\n",
      "There are 25572 words in book6.txt.\n",
      "There are 12161 words in book8.txt.\n",
      "There are 100649 words in book9.txt.\n",
      "There are 69693 words in book10.txt.\n"
     ]
    }
   ],
   "source": [
    "books = load_data(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book(book):\n",
    "    \"\"\"\n",
    "        Remove unwanted special characters\n",
    "        and extra spaces from the text.\n",
    "        \n",
    "        Input:\n",
    "            list: List of books\n",
    "        Output:\n",
    "            list: Cleaned list of books\n",
    "    \"\"\"\n",
    "    book = re.sub(r'\\n', ' ', book) \n",
    "    book = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', book)\n",
    "    book = re.sub('a0','', book)\n",
    "    book = re.sub('\\'92t','\\'t', book)\n",
    "    book = re.sub('\\'92s','\\'s', book)\n",
    "    book = re.sub('\\'92m','\\'m', book)\n",
    "    book = re.sub('\\'92ll','\\'ll', book)\n",
    "    book = re.sub('\\'91','', book)\n",
    "    book = re.sub('\\'92','', book)\n",
    "    book = re.sub('\\'93','', book)\n",
    "    book = re.sub('\\'94','', book)\n",
    "    book = re.sub('\\.','. ', book)\n",
    "    book = re.sub('\\!','! ', book)\n",
    "    book = re.sub('\\?','? ', book)\n",
    "    book = re.sub(' +',' ', book)\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean books\n",
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_book(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to convert the vocabulary (characters) to integers\n",
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 88 characters.\n",
      "[' ', '!', '$', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'â', 'æ', 'è', 'é', 'ê', 'ô', 'œ', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Check the size of vocabulary and all of the values\n",
    "vocab_size = len(vocab_to_int)\n",
    "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
    "print(sorted(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int to vocab\n",
    "int_to_vocab = {}\n",
    "for character, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35662 sentences\n"
     ]
    }
   ],
   "source": [
    "# Create sentences from the books\n",
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split(\". \"):\n",
    "        sentences.append(sentence + \".\")\n",
    "print(\"There are {} sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert sentences to integers\n",
    "# int_sentences = []\n",
    "# for sentence in sentences:\n",
    "#     int_sentence = []\n",
    "#     for character in sentence:\n",
    "#         int_sentence.append(vocab_to_int[character])\n",
    "#     int_sentences.append(int_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 15298 sentences to train and test our model.\n"
     ]
    }
   ],
   "source": [
    "# Limit data for training\n",
    "max_length = 92\n",
    "min_length = 10\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "print(\"We will use {} sentences to train and test our model.\".format(len(good_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise maker (input sequences)\n",
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    \n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_int = np.random.randint(0, len(int_to_vocab)-1)\n",
    "                noisy_sentence.append(int_to_vocab[random_int])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspirited by this wind of promise, my daydreams become more fervent and vivid.\n",
      "Inspirited y this windzoCf promWies”, ym ayrdeams become more fervent and ivvid.\n",
      "\n",
      "This expedition has been the favourite dream of my early years.\n",
      "This expeditijon ha bee nthe !favourite dream of my early years2.\n",
      "\n",
      "My education was neglected, yet I was passionately fond of reading.\n",
      "Myeucation as nglected, yet I  was passonately Df—ond ofreading.\n",
      "\n",
      "You are well acquainted with my failure and how heavily I bore the disappointment.\n",
      "You are wel cauqian—ted with my failuey ?and hoqw heaviy I bore the disapointment.\n",
      "\n",
      "Six years have passed since I resolved on my present undertaking.\n",
      "Six ears hve passd since I resoved on m`y persent undert&aking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure noise_maker is making mistakes correctly.\n",
    "threshold = 0.9\n",
    "for sentence in good_sentences[:5]:\n",
    "    print(sentence)\n",
    "    print(\"\".join(noise_maker(sentence, threshold)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and target text\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Shuffle the list of good_senteces\n",
    "np.random.shuffle(good_sentences)\n",
    "\n",
    "# Split list of good sentences into\n",
    "# training and testing sets\n",
    "# training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
    "\n",
    "# Pad target sentences and build\n",
    "# input and target vocabulary\n",
    "for sentence in good_sentences:\n",
    "    input_text = \"\".join(noise_maker(sentence, 0.9))\n",
    "    target_text = \"{}{}{}\".format(\"\\t\", sentence , \"\\n\")\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Set up our encoder and decoder inputs\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# Logging\n",
    "# print('Number of samples:', len(input_texts))\n",
    "# print('Number of unique input tokens:', num_encoder_tokens)\n",
    "# print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# Create dicts for the input and\n",
    "# target vocabolaries\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Crete vectorize data placeholders \n",
    "# for encoder and decoder\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate tensors\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5060 samples, validate on 1266 samples\n",
      "Epoch 1/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 1.8130 - acc: 0.0984 - val_loss: 1.7430 - val_acc: 0.1077\n",
      "Epoch 2/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 1.6589 - acc: 0.1253 - val_loss: 1.5573 - val_acc: 0.1481\n",
      "Epoch 3/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 1.4828 - acc: 0.1739 - val_loss: 1.4367 - val_acc: 0.1725\n",
      "Epoch 4/100\n",
      "5060/5060 [==============================] - 80s 16ms/step - loss: 1.3744 - acc: 0.1933 - val_loss: 1.3589 - val_acc: 0.1841\n",
      "Epoch 5/100\n",
      "5060/5060 [==============================] - 83s 16ms/step - loss: 1.3092 - acc: 0.2070 - val_loss: 1.3085 - val_acc: 0.1969\n",
      "Epoch 6/100\n",
      "5060/5060 [==============================] - 79s 16ms/step - loss: 1.2591 - acc: 0.2192 - val_loss: 1.2504 - val_acc: 0.2125\n",
      "Epoch 7/100\n",
      "5060/5060 [==============================] - 78s 15ms/step - loss: 1.2194 - acc: 0.2292 - val_loss: 1.2297 - val_acc: 0.2128\n",
      "Epoch 8/100\n",
      "5060/5060 [==============================] - 85s 17ms/step - loss: 1.1808 - acc: 0.2378 - val_loss: 1.1920 - val_acc: 0.2242\n",
      "Epoch 9/100\n",
      "5060/5060 [==============================] - 66s 13ms/step - loss: 1.1497 - acc: 0.2449 - val_loss: 1.1580 - val_acc: 0.2332\n",
      "Epoch 10/100\n",
      "5060/5060 [==============================] - 86s 17ms/step - loss: 1.1264 - acc: 0.2503 - val_loss: 1.1332 - val_acc: 0.2385\n",
      "Epoch 11/100\n",
      "5060/5060 [==============================] - 75s 15ms/step - loss: 1.0946 - acc: 0.2581 - val_loss: 1.1369 - val_acc: 0.2387\n",
      "Epoch 12/100\n",
      "5060/5060 [==============================] - 77s 15ms/step - loss: 1.0697 - acc: 0.2643 - val_loss: 1.1281 - val_acc: 0.2402\n",
      "Epoch 13/100\n",
      "5060/5060 [==============================] - 84s 17ms/step - loss: 1.0585 - acc: 0.2671 - val_loss: 1.0719 - val_acc: 0.2538\n",
      "Epoch 14/100\n",
      "5060/5060 [==============================] - 67s 13ms/step - loss: 1.0338 - acc: 0.2729 - val_loss: 1.0915 - val_acc: 0.2485\n",
      "Epoch 15/100\n",
      "5060/5060 [==============================] - 76s 15ms/step - loss: 1.0167 - acc: 0.2778 - val_loss: 1.0743 - val_acc: 0.2544\n",
      "Epoch 16/100\n",
      "5060/5060 [==============================] - 78s 15ms/step - loss: 0.9952 - acc: 0.2834 - val_loss: 1.0293 - val_acc: 0.2628\n",
      "Epoch 17/100\n",
      "5060/5060 [==============================] - 78s 15ms/step - loss: 0.9769 - acc: 0.2879 - val_loss: 0.9972 - val_acc: 0.2735\n",
      "Epoch 18/100\n",
      "5060/5060 [==============================] - 73s 14ms/step - loss: 0.9595 - acc: 0.2924 - val_loss: 1.0064 - val_acc: 0.2707\n",
      "Epoch 19/100\n",
      "5060/5060 [==============================] - 81s 16ms/step - loss: 0.9441 - acc: 0.2964 - val_loss: 0.9880 - val_acc: 0.2761\n",
      "Epoch 20/100\n",
      "5060/5060 [==============================] - 64s 13ms/step - loss: 0.9293 - acc: 0.3005 - val_loss: 0.9808 - val_acc: 0.2775\n",
      "Epoch 21/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 0.9151 - acc: 0.3045 - val_loss: 0.9567 - val_acc: 0.2861\n",
      "Epoch 22/100\n",
      "5060/5060 [==============================] - 95s 19ms/step - loss: 0.9022 - acc: 0.3084 - val_loss: 0.9565 - val_acc: 0.2831\n",
      "Epoch 23/100\n",
      "5060/5060 [==============================] - 116s 23ms/step - loss: 0.8895 - acc: 0.3122 - val_loss: 0.9743 - val_acc: 0.2778\n",
      "Epoch 24/100\n",
      "5060/5060 [==============================] - 141s 28ms/step - loss: 0.8785 - acc: 0.3149 - val_loss: 0.9345 - val_acc: 0.2911\n",
      "Epoch 25/100\n",
      "5060/5060 [==============================] - 90s 18ms/step - loss: 0.8666 - acc: 0.3182 - val_loss: 0.9279 - val_acc: 0.2914\n",
      "Epoch 26/100\n",
      "5060/5060 [==============================] - 82s 16ms/step - loss: 0.8553 - acc: 0.3208 - val_loss: 0.9308 - val_acc: 0.2916\n",
      "Epoch 27/100\n",
      "5060/5060 [==============================] - 87s 17ms/step - loss: 0.8455 - acc: 0.3242 - val_loss: 0.9251 - val_acc: 0.2922\n",
      "Epoch 28/100\n",
      "5060/5060 [==============================] - 81s 16ms/step - loss: 0.8349 - acc: 0.3269 - val_loss: 0.9257 - val_acc: 0.2921\n",
      "Epoch 29/100\n",
      "5060/5060 [==============================] - 57s 11ms/step - loss: 0.8248 - acc: 0.3302 - val_loss: 0.9213 - val_acc: 0.2938\n",
      "Epoch 30/100\n",
      "5060/5060 [==============================] - 75s 15ms/step - loss: 0.8154 - acc: 0.3322 - val_loss: 0.9040 - val_acc: 0.3004\n",
      "Epoch 31/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 0.8060 - acc: 0.3347 - val_loss: 0.9107 - val_acc: 0.2975\n",
      "Epoch 32/100\n",
      "5060/5060 [==============================] - 80s 16ms/step - loss: 0.7977 - acc: 0.3372 - val_loss: 0.8918 - val_acc: 0.3039\n",
      "Epoch 33/100\n",
      "5060/5060 [==============================] - 67s 13ms/step - loss: 0.7891 - acc: 0.3393 - val_loss: 0.8982 - val_acc: 0.3005\n",
      "Epoch 34/100\n",
      "5060/5060 [==============================] - 71s 14ms/step - loss: 0.7801 - acc: 0.3426 - val_loss: 0.8936 - val_acc: 0.3014\n",
      "Epoch 35/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 0.7720 - acc: 0.3448 - val_loss: 0.8999 - val_acc: 0.3013\n",
      "Epoch 36/100\n",
      "5060/5060 [==============================] - 70s 14ms/step - loss: 0.7637 - acc: 0.3473 - val_loss: 0.8924 - val_acc: 0.3043\n",
      "Epoch 37/100\n",
      "5060/5060 [==============================] - 70s 14ms/step - loss: 0.7561 - acc: 0.3492 - val_loss: 0.8956 - val_acc: 0.3001\n",
      "Epoch 38/100\n",
      "5060/5060 [==============================] - 67s 13ms/step - loss: 0.7484 - acc: 0.3513 - val_loss: 0.9064 - val_acc: 0.2994\n",
      "Epoch 39/100\n",
      "5060/5060 [==============================] - 75s 15ms/step - loss: 0.7407 - acc: 0.3538 - val_loss: 0.8949 - val_acc: 0.3044\n",
      "Epoch 40/100\n",
      "5060/5060 [==============================] - 66s 13ms/step - loss: 0.7335 - acc: 0.3559 - val_loss: 0.8843 - val_acc: 0.3074\n",
      "Epoch 41/100\n",
      "5060/5060 [==============================] - 63s 12ms/step - loss: 0.7255 - acc: 0.3578 - val_loss: 0.8721 - val_acc: 0.3120\n",
      "Epoch 42/100\n",
      "5060/5060 [==============================] - 67s 13ms/step - loss: 0.7184 - acc: 0.3601 - val_loss: 0.8967 - val_acc: 0.3021\n",
      "Epoch 43/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 0.7110 - acc: 0.3620 - val_loss: 0.8789 - val_acc: 0.3095\n",
      "Epoch 44/100\n",
      "5060/5060 [==============================] - 60s 12ms/step - loss: 0.7045 - acc: 0.3638 - val_loss: 0.8810 - val_acc: 0.3089\n",
      "Epoch 45/100\n",
      "5060/5060 [==============================] - 63s 12ms/step - loss: 0.6978 - acc: 0.3660 - val_loss: 0.8834 - val_acc: 0.3081\n",
      "Epoch 46/100\n",
      "5060/5060 [==============================] - 63s 12ms/step - loss: 0.6909 - acc: 0.3678 - val_loss: 0.8975 - val_acc: 0.3054\n",
      "Epoch 47/100\n",
      "5060/5060 [==============================] - 63s 12ms/step - loss: 0.6839 - acc: 0.3701 - val_loss: 0.9038 - val_acc: 0.3048\n",
      "Epoch 48/100\n",
      "5060/5060 [==============================] - 79s 16ms/step - loss: 0.6772 - acc: 0.3723 - val_loss: 0.8979 - val_acc: 0.3067\n",
      "Epoch 49/100\n",
      "5060/5060 [==============================] - 79s 16ms/step - loss: 0.6710 - acc: 0.3739 - val_loss: 0.8918 - val_acc: 0.3100\n",
      "Epoch 50/100\n",
      "5060/5060 [==============================] - 73s 14ms/step - loss: 0.6638 - acc: 0.3761 - val_loss: 0.8954 - val_acc: 0.3085\n",
      "Epoch 51/100\n",
      "5060/5060 [==============================] - 67s 13ms/step - loss: 0.6575 - acc: 0.3781 - val_loss: 0.9033 - val_acc: 0.3082\n",
      "Epoch 52/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.6514 - acc: 0.3792 - val_loss: 0.8987 - val_acc: 0.3088\n",
      "Epoch 53/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.6446 - acc: 0.3816 - val_loss: 0.9052 - val_acc: 0.3079\n",
      "Epoch 54/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.6387 - acc: 0.3832 - val_loss: 0.9130 - val_acc: 0.3048\n",
      "Epoch 55/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.6317 - acc: 0.3856 - val_loss: 0.9222 - val_acc: 0.3027\n",
      "Epoch 56/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.6267 - acc: 0.3870 - val_loss: 0.9116 - val_acc: 0.3068\n",
      "Epoch 57/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.6200 - acc: 0.3895 - val_loss: 0.9289 - val_acc: 0.3048\n",
      "Epoch 58/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.6144 - acc: 0.3905 - val_loss: 0.9183 - val_acc: 0.3071\n",
      "Epoch 59/100\n",
      "5060/5060 [==============================] - 679s 134ms/step - loss: 0.6076 - acc: 0.3931 - val_loss: 0.9134 - val_acc: 0.3082\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.6021 - acc: 0.3947 - val_loss: 0.9314 - val_acc: 0.3048\n",
      "Epoch 61/100\n",
      "5060/5060 [==============================] - 57s 11ms/step - loss: 0.5970 - acc: 0.3957 - val_loss: 0.9282 - val_acc: 0.3072\n",
      "Epoch 62/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.5909 - acc: 0.3977 - val_loss: 0.9302 - val_acc: 0.3064\n",
      "Epoch 63/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.5850 - acc: 0.3995 - val_loss: 0.9384 - val_acc: 0.3042\n",
      "Epoch 64/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.5811 - acc: 0.4007 - val_loss: 0.9436 - val_acc: 0.3043\n",
      "Epoch 65/100\n",
      "5060/5060 [==============================] - 848s 168ms/step - loss: 0.5744 - acc: 0.4024 - val_loss: 0.9485 - val_acc: 0.3047\n",
      "Epoch 66/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5955 - acc: 0.3970 - val_loss: 0.9654 - val_acc: 0.3006\n",
      "Epoch 67/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5823 - acc: 0.4001 - val_loss: 0.9730 - val_acc: 0.2987\n",
      "Epoch 68/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5714 - acc: 0.4036 - val_loss: 0.9497 - val_acc: 0.3036\n",
      "Epoch 69/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5606 - acc: 0.4069 - val_loss: 0.9530 - val_acc: 0.3047\n",
      "Epoch 70/100\n",
      "5060/5060 [==============================] - 71s 14ms/step - loss: 0.5552 - acc: 0.4080 - val_loss: 0.9638 - val_acc: 0.3012\n",
      "Epoch 71/100\n",
      "5060/5060 [==============================] - 68s 13ms/step - loss: 0.5504 - acc: 0.4098 - val_loss: 0.9691 - val_acc: 0.3011\n",
      "Epoch 72/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5424 - acc: 0.4124 - val_loss: 0.9823 - val_acc: 0.3007\n",
      "Epoch 73/100\n",
      "5060/5060 [==============================] - 69s 14ms/step - loss: 0.5379 - acc: 0.4139 - val_loss: 0.9795 - val_acc: 0.3027\n",
      "Epoch 74/100\n",
      "5060/5060 [==============================] - 76s 15ms/step - loss: 0.5305 - acc: 0.4160 - val_loss: 0.9917 - val_acc: 0.2985\n",
      "Epoch 75/100\n",
      "5060/5060 [==============================] - 77s 15ms/step - loss: 0.5271 - acc: 0.4166 - val_loss: 0.9951 - val_acc: 0.2997\n",
      "Epoch 76/100\n",
      "5060/5060 [==============================] - 72s 14ms/step - loss: 0.5219 - acc: 0.4183 - val_loss: 1.0180 - val_acc: 0.2954\n",
      "Epoch 77/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.5172 - acc: 0.4195 - val_loss: 1.0079 - val_acc: 0.2991\n",
      "Epoch 78/100\n",
      "5060/5060 [==============================] - 58s 11ms/step - loss: 0.5116 - acc: 0.4213 - val_loss: 1.0142 - val_acc: 0.2991\n",
      "Epoch 79/100\n",
      "5060/5060 [==============================] - 57s 11ms/step - loss: 0.5072 - acc: 0.4230 - val_loss: 1.0206 - val_acc: 0.2978\n",
      "Epoch 80/100\n",
      "5060/5060 [==============================] - 57s 11ms/step - loss: 0.5013 - acc: 0.4249 - val_loss: 1.0203 - val_acc: 0.3006\n",
      "Epoch 81/100\n",
      "5060/5060 [==============================] - 57s 11ms/step - loss: 0.4977 - acc: 0.4258 - val_loss: 1.0222 - val_acc: 0.2981\n",
      "Epoch 82/100\n",
      "5060/5060 [==============================] - 73s 14ms/step - loss: 0.4943 - acc: 0.4266 - val_loss: 1.0244 - val_acc: 0.3001\n",
      "Epoch 83/100\n",
      "5060/5060 [==============================] - 81s 16ms/step - loss: 0.4896 - acc: 0.4284 - val_loss: 1.0225 - val_acc: 0.3018\n",
      "Epoch 84/100\n",
      "5060/5060 [==============================] - 77s 15ms/step - loss: 0.4855 - acc: 0.4294 - val_loss: 1.0427 - val_acc: 0.2983\n",
      "Epoch 85/100\n",
      "5060/5060 [==============================] - 75s 15ms/step - loss: 0.4810 - acc: 0.4310 - val_loss: 1.0452 - val_acc: 0.2950\n",
      "Epoch 86/100\n",
      "5060/5060 [==============================] - 66s 13ms/step - loss: 0.4769 - acc: 0.4321 - val_loss: 1.0375 - val_acc: 0.2993\n",
      "Epoch 87/100\n",
      "5060/5060 [==============================] - 58s 12ms/step - loss: 0.4740 - acc: 0.4329 - val_loss: 1.0654 - val_acc: 0.2958\n",
      "Epoch 88/100\n",
      "5060/5060 [==============================] - 68s 14ms/step - loss: 0.4685 - acc: 0.4347 - val_loss: 1.0560 - val_acc: 0.2984\n",
      "Epoch 89/100\n",
      "5060/5060 [==============================] - 68s 13ms/step - loss: 0.4661 - acc: 0.4352 - val_loss: 1.0693 - val_acc: 0.2951\n",
      "Epoch 90/100\n",
      "5060/5060 [==============================] - 77s 15ms/step - loss: 0.4620 - acc: 0.4366 - val_loss: 1.0613 - val_acc: 0.2955\n",
      "Epoch 91/100\n",
      "5060/5060 [==============================] - 81s 16ms/step - loss: 0.4583 - acc: 0.4379 - val_loss: 1.0652 - val_acc: 0.2978\n",
      "Epoch 92/100\n",
      "5060/5060 [==============================] - 70s 14ms/step - loss: 0.4546 - acc: 0.4388 - val_loss: 1.0742 - val_acc: 0.2965\n",
      "Epoch 93/100\n",
      "5060/5060 [==============================] - 76s 15ms/step - loss: 0.4515 - acc: 0.4400 - val_loss: 1.0745 - val_acc: 0.2970\n",
      "Epoch 94/100\n",
      "5060/5060 [==============================] - 75s 15ms/step - loss: 0.4471 - acc: 0.4408 - val_loss: 1.0782 - val_acc: 0.2980\n",
      "Epoch 95/100\n",
      "5060/5060 [==============================] - 81s 16ms/step - loss: 0.4446 - acc: 0.4416 - val_loss: 1.0899 - val_acc: 0.2942\n",
      "Epoch 96/100\n",
      "5060/5060 [==============================] - 74s 15ms/step - loss: 0.4420 - acc: 0.4425 - val_loss: 1.1044 - val_acc: 0.2931\n",
      "Epoch 97/100\n",
      "5060/5060 [==============================] - 84s 17ms/step - loss: 0.4379 - acc: 0.4437 - val_loss: 1.1042 - val_acc: 0.2953\n",
      "Epoch 98/100\n",
      "5060/5060 [==============================] - 79s 16ms/step - loss: 0.4351 - acc: 0.4443 - val_loss: 1.0986 - val_acc: 0.2967\n",
      "Epoch 99/100\n",
      "5060/5060 [==============================] - 68s 14ms/step - loss: 0.4319 - acc: 0.4453 - val_loss: 1.1102 - val_acc: 0.2935\n",
      "Epoch 100/100\n",
      "5060/5060 [==============================] - 91s 18ms/step - loss: 0.4277 - acc: 0.4465 - val_loss: 1.1167 - val_acc: 0.2946\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, 101)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "epochs = range(1, 101)\n",
    "acc_values = history_dict['acc'] \n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dailand10/Desktop/dlbaby/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('charSeq2Seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer lstm_4 expects 7 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'input_6:0' shape=(?, ?, 81) dtype=float32>, <tf.Tensor 'input_13:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_14:0' shape=(?, 256) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-d8ba0444ce79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m decoder_outputs, state_h, state_c = decoder_lstm(\n\u001b[0;32m---> 16\u001b[0;31m     decoder_inputs, initial_state=decoder_states_inputs)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdecoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dlbaby/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dlbaby/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dlbaby/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                              \u001b[0;34m'but it received '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                              \u001b[0;34m' input tensors. Input received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                              str(inputs))\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer lstm_4 expects 7 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'input_6:0' shape=(?, ?, 81) dtype=float32>, <tf.Tensor 'input_13:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_14:0' shape=(?, 256) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: T te finny people’s king.\n",
      "Decoded sentence: Bennet was really in the strent.\n",
      "\n",
      "-\n",
      "Input sentence: ” “Anothert ime, Lizzy,” sid e rmother,“ I would not dance ith him if I were you\n",
      "Decoded sentence: ” “How certainly sake healtiness the sivents of a facture there, I do not.\n",
      "\n",
      "-\n",
      "Input sentence: She cAomes to usè to-day.\n",
      "Decoded sentence: She was a screary of never.\n",
      "\n",
      "-\n",
      "Input sentence: ” “Tfhe county,” siad Darcy, “cani”n geneural supplb Vua few subjecs for such a stud.\n",
      "Decoded sentence: ” “Haul in the curious of the way what he was sitter on that way yourself.\n",
      "\n",
      "-\n",
      "Input sentence: ” “Dear madam,cried Mrs.\n",
      "Decoded sentence: ” “He isen my by.\n",
      "\n",
      "-\n",
      "Input sentence: I lov Elizabeth and lookO forward to our union with deUligh!t.\n",
      "Decoded sentence: I don’t know what I had been the gleade still attention.\n",
      "\n",
      "-\n",
      "Input sentence: ollins !” “My dYear madam,W” repli2ed he, “let us befor ev’er silento n hts point.\n",
      "Decoded sentence: Come on die any hand of your own shild be forney is only time a few constann.\n",
      "\n",
      "-\n",
      "Input sentence: “Not atê all.\n",
      "Decoded sentence: “Now, sir.\n",
      "\n",
      "-\n",
      "Input sentence: ’B‘ut what am I to do? ’ said Alice.\n",
      "Decoded sentence: ’ “‘How’v thing the whale’s ord.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him use it not a week ago.\n",
      "Decoded sentence: I do not recolved the generas.\n",
      "\n",
      "-\n",
      "Input sentence: “Boks-o-h! no.\n",
      "Decoded sentence: “Your a letter.\n",
      "\n",
      "-\n",
      "Input sentence: The enviousillows sdielogn wsell to whelm my tr;akc; let them; ubt first I pass.\n",
      "Decoded sentence: The same manners were put our for the resionaly of my earduage from Mr.\n",
      "\n",
      "-\n",
      "Input sentence: I sayt o, becaus ethe sat eof my own knowlegdOe does not pass beIyond that point4.\n",
      "Decoded sentence: I travelleced my friend, and then tood placed with his deetimeresion.\n",
      "\n",
      "-\n",
      "Input sentence: ‘Nr I,’ said t0he aMrch Ha4re.\n",
      "Decoded sentence: ‘No, no! ’ said the Macch.\n",
      "\n",
      "-\n",
      "Input sentence: ” Then Nwatchni gthôe amte’s —countennec,he adkded, “Teh steGward Mr.\n",
      "Decoded sentence: ” The night which it was a fair for the name in the same proulsain.\n",
      "\n",
      "-\n",
      "Input sentence: IW mntioUned i nmy last leter theferas I entertained ofa zmuiyn.\n",
      "Decoded sentence: I am sure you will not believe that you would have been along mean.\n",
      "\n",
      "-\n",
      "Input sentence: OVf the Less Erroneou Picturesof Whaes, andthe TruePictrse of Whaling Scenes.\n",
      "Decoded sentence: Stubb’s allore that I had not been very intr fur this windon hish of the dismate.\n",
      "\n",
      "-\n",
      "Input sentence: I no omre fle!t unuly” cnocerne dofr the landlodr’s polciy of insurance.\n",
      "Decoded sentence: I am sure you will not be id rutting and greater is never sail yours.\n",
      "\n",
      "-\n",
      "Input sentence: o stOock fish.\n",
      "Decoded sentence: So the last entered.\n",
      "\n",
      "-\n",
      "Input sentence: But I—I& 0have lost everything an dcannot bgin ôliffe naew\n",
      "Decoded sentence: But there is another the gleasents of same for you.\n",
      "\n",
      "-\n",
      "Input sentence: adinre, Wickham had oen means hof affording pleasure, unconnected with ihs gevnerael powesrs.\n",
      "Decoded sentence: Let us allown thungerstanting at the spare, and the morning of the seasons are setreate.\n",
      "\n",
      "-\n",
      "Input sentence: Perhaps hethought her etoo young.\n",
      "Decoded sentence: She was a screaty of all a mind.\n",
      "\n",
      "-\n",
      "Input sentence: There can be n occaison for your ging so1 soon.\n",
      "Decoded sentence: The poor was sucheady to herself, say it was.\n",
      "\n",
      "-\n",
      "Input sentence: It was t SiWr W6illiam Lucas's, hwerea large party were assemb9le.\n",
      "Decoded sentence: If I cannot was concerted from him, and stanged to Mrs.\n",
      "\n",
      "-\n",
      "Input sentence: Mrak this, too, in hte wMhaler.\n",
      "Decoded sentence: My dearest heat in the shir.\n",
      "\n",
      "-\n",
      "Input sentence: Others ofthe party were now palpiedi to.\n",
      "Decoded sentence: A short may be the seasoness of Mr.\n",
      "\n",
      "-\n",
      "Input sentence: Like curs liket; and fo&r thi shunt, my malady becomes my most desired health.\n",
      "Decoded sentence: Lady Catherine resent of me no comfully to make of him an answers of Mr.\n",
      "\n",
      "-\n",
      "Input sentence: Msis NBingley, I am sure cannt.\n",
      "Decoded sentence: Jane was a connect of little conders.\n",
      "\n",
      "-\n",
      "Input sentence: Hyde! S” “Ay ay,” said the lawhy.\n",
      "Decoded sentence: Hyde at as a succeed with.\n",
      "\n",
      "-\n",
      "Input sentence: ‘Ten hours the first day,’ said th eMock Ttrel: ‘nine the next,a nd soo n.\n",
      "Decoded sentence: ‘What a curious all for a famed of my idnewne? ’ ‘It what Mr.\n",
      "\n",
      "-\n",
      "Input sentence: Knihts and Squies.\n",
      "Decoded sentence: She was liked me.\n",
      "\n",
      "-\n",
      "Input sentence: Here also wem ade soe acquaintances, Lwhoalmost conrivedt ocheat me into hppiness.\n",
      "Decoded sentence: He has not a werk to read the contracting secone of the whale, and Mrs.\n",
      "\n",
      "-\n",
      "Input sentence: The bn isworking loose and thew lee lift s ahClf-stranded.\n",
      "Decoded sentence: The same manners were pussiously and discoissoed him.\n",
      "\n",
      "-\n",
      "Input sentence: ”“ ll, sir, ant to see what whaling is.\n",
      "Decoded sentence: ” “You are very gloasedy to then.\n",
      "\n",
      "-\n",
      "Input sentence:  Ientered the room where th ecorpse layk and was led up ot the coffni.\n",
      "Decoded sentence: In hat though there our in a when reased with the diecess of Mr.\n",
      "\n",
      "-\n",
      "Input sentence: Thoughn om6inallyi ncluded in the Ncnsus of Chriastendom he was tsill a laien t oit.\n",
      "Decoded sentence: Thought neass the send says te house it would have been in one the sea offerdance.\n",
      "\n",
      "-\n",
      "Input sentence: ” yiaœ declared hesrelf satisfie.\n",
      "Decoded sentence: ” The day of hours like it.\n",
      "\n",
      "-\n",
      "Input sentence: , whSo,i nstantly ta—king hte aarm, setN off from B.\n",
      "Decoded sentence: Elizabeth was not long remarking a concersion.\n",
      "\n",
      "-\n",
      "Input sentence: “Whale’s in thce sea God’s voic oby.\n",
      "Decoded sentence: “What do you want? ” “Yes, he may.\n",
      "\n",
      "-\n",
      "Input sentence: Crossing the dekcA,l et us snow rhave a ogod long look at the Righ Whale’s Ahead.\n",
      "Decoded sentence: For the last teer was the subject suched it an whale-may, and they were.\n",
      "\n",
      "-\n",
      "Input sentence: “He’s giongto sound!In stnsails! Down top-galant-smaiDls! Stand by thre baots.\n",
      "Decoded sentence: “He works,” said he, “you know them shall be no commentation to the whale.\n",
      "\n",
      "-\n",
      "Input sentence: H ewas storing ihsV memory with anedcotesa nd noble names.\n",
      "Decoded sentence: Bingley and his coming who feet along attention of any hope.\n",
      "\n",
      "-\n",
      "Input sentence:  ”Soon afte rwe hear dthKa thep oor victi mhad expressed a desireto see my ceousin.\n",
      "Decoded sentence: ’ ‘What dugnen’s now a sea--and pape there,’ said the Caterpillar she while.\n",
      "\n",
      "-\n",
      "Input sentence: Toc ross to the other sfid of hed èeck was like walking up the steep gabled roof fo a house.\n",
      "Decoded sentence: Collins was will by the master whales of the cares of a finity of a humber and sail.\n",
      "\n",
      "-\n",
      "Input sentence: he figurewas stiff; ub the signature was good for more than téhati f itw s only genuine.\n",
      "Decoded sentence: When the short should set stild to love in the carriage with something of she is.\n",
      "\n",
      "-\n",
      "Input sentence: In thh al the yparted.\n",
      "Decoded sentence: In the ship stoulds of Miss De.\n",
      "\n",
      "-\n",
      "Input sentence: ” “I shouVld tapke hi,m even on my slight acquaintan,e tNo- be an ill-tem-peerd man.\n",
      "Decoded sentence: ” “I do not think I have now comfust it that you that he had tell me for her.\n",
      "\n",
      "-\n",
      "Input sentence: But wha5tever may be thes entimets whihc Mr.\n",
      "Decoded sentence: But he was silent an whalenging spent.\n",
      "\n",
      "-\n",
      "Input sentence:  ”She rpeliâed izn the affirmative.\n",
      "Decoded sentence: ” The diffirm tood herself.\n",
      "\n",
      "-\n",
      "Input sentence: Fro mthi sday you mstbe' a stranger to noe ofyour0 paernts.\n",
      "Decoded sentence: From thence they were for every great friends.\n",
      "\n",
      "-\n",
      "Input sentence: ’ “S‘hall we?’ cried th ringleader to his meCn.\n",
      "Decoded sentence: ’ “‘I do’t true my foung of any of sair.\n",
      "\n",
      "-\n",
      "Input sentence: But I lay perfectly still andr esolve dnot to sya a wordt ill spoken” to.\n",
      "Decoded sentence: But there is another the getting of a mass-eleage and the subjer.\n",
      "\n",
      "-\n",
      "Input sentence: A Loos-eFish: is fair game fora nygbody who can soonest catch it.\n",
      "Decoded sentence: Alice looked at her father and his offen in charming whence.\n",
      "\n",
      "-\n",
      "Input sentence: Her eae and good spirits increasegd.\n",
      "Decoded sentence: He was not the best it doen in one.\n",
      "\n",
      "-\n",
      "Input sentence: “‘Shut u u pagain, will y!e ’ crLiedS teelkilt.\n",
      "Decoded sentence: “He must be a summert the get of theme.\n",
      "\n",
      "-\n",
      "Input sentence:  “Stop! ” c8ried the strangre.\n",
      "Decoded sentence: ” “I cannot be so disprise.\n",
      "\n",
      "-\n",
      "Input sentence: But only by thSe replacig of the cap Kw sthta omen accountedgood.\n",
      "Decoded sentence: But there is another young man to be the greatest of pretand.\n",
      "\n",
      "-\n",
      "Input sentence: Elizaebth too kleave oft h whole party i nthe liveliest of spirits.\n",
      "Decoded sentence: Elizabeth was surprised, but seemsh the mandscembs he was risines.\n",
      "\n",
      "-\n",
      "Input sentence: Uttesonè :stepped out and otuhed him on the shoulder £as he passed.\n",
      "Decoded sentence: Utterson storp heresh when you would pat te most seams but compression.\n",
      "\n",
      "-\n",
      "Input sentence: Cuttign? In.\n",
      "Decoded sentence: CHAPTER 105.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: “Back,l ad; I will be with ye agaibn presently.\n",
      "Decoded sentence: “Yes, sir, in the bellowed him of his hand.\n",
      "\n",
      "-\n",
      "Input sentence: If I should be engaged,B I wil atD least bmake notes.\n",
      "Decoded sentence: I found there is a home they are to get it.\n",
      "\n",
      "-\n",
      "Input sentence: I olokde upon them as uspeLrior beingsw ho wouldbe thnea rbiters of my future desitn.y\n",
      "Decoded sentence: I travelleced my friend, and then tood placed with his deetimeresion, you know.\n",
      "\n",
      "-\n",
      "Input sentence: Snodhead,I tanscriebt he folowing: 4030,000 lbs.\n",
      "Decoded sentence: Since you leffore to the contract offen.\n",
      "\n",
      "-\n",
      "Input sentence: I roused, and in“tersetdyuo, bceause I Rwas svo unlike them.\n",
      "Decoded sentence: I don’t know how it is the fiesure of her sister's weak.\n",
      "\n",
      "-\n",
      "Input sentence: An invia?tionfro m he ueet o pêlay rcoquet.\n",
      "Decoded sentence: And they deen helf himport of it.\n",
      "\n",
      "-\n",
      "Input sentence: “It sees Ksacrcely a houseu.\n",
      "Decoded sentence: “I wish I could so.\n",
      "\n",
      "-\n",
      "Input sentence: NH office is veer to enter into myh ousel agaXin, no reven to êpass through the villgae.\n",
      "Decoded sentence: Do your father a share that is his? ” “Oh, norring! we seeker-him, and satings.\n",
      "\n",
      "-\n",
      "Input sentence: Thée Chase.\n",
      "Decoded sentence: The Cartent.\n",
      "\n",
      "-\n",
      "Input sentence: W“ell,” saidE nfiel, “thta stroy’s ;at any nd at lest.\n",
      "Decoded sentence: “Why drenged prop be disers, in the pastage of Mr.\n",
      "\n",
      "-\n",
      "Input sentence: œThereinZ no fairy’s armc can transcnd it.\n",
      "Decoded sentence: The poor was sucheady to the deemest.\n",
      "\n",
      "-\n",
      "Input sentence: ” “Youm ay asC wQellcallm t meprtinence at onceu.\n",
      "Decoded sentence: ” “You are very gloased of it? ” “I never mare to Mr.\n",
      "\n",
      "-\n",
      "Input sentence: CHAPTER 9.\n",
      "Decoded sentence: CHAPTER 66.\n",
      "\n",
      "-\n",
      "Input sentence: I sruvvied mysel;f my deat hand burial were locked up in m ychest.\n",
      "Decoded sentence: I don’t know which I had been stepped for him in a way ago.\n",
      "\n",
      "-\n",
      "Input sentence: Tey are fightinpg Quakers; they are Quakers with a vengeanc.e\n",
      "Decoded sentence: The same manners were pussiously and discoissoed him.\n",
      "\n",
      "-\n",
      "Input sentence: CHAPTRE 76.\n",
      "Decoded sentence: CHAPTER 66.\n",
      "\n",
      "-\n",
      "Input sentence: ollin, all prase of me wil b unnecesæsary.\n",
      "Decoded sentence: Collins to be conficers into the carriane.\n",
      "\n",
      "-\n",
      "Input sentence: He has no proper nose.\n",
      "Decoded sentence: He was not the letter.\n",
      "\n",
      "-\n",
      "Input sentence: ‘Then you should say whèat you mean,’the 8Marhc Hare went on.\n",
      "Decoded sentence: ‘What a curious all for my awn great’s head is a pleaser.\n",
      "\n",
      "-\n",
      "Input sentence: Kitty adn mew eret o spned the ady htere, nd Mrs.\n",
      "Decoded sentence: Since you left us it it; and solvery who came.\n",
      "\n",
      "-\n",
      "Input sentence: Bennet; “and if I were ots ee you at it,  Ishold taek away your bottledirectly.\n",
      "Decoded sentence: Bennet was ready, and they shall never are going of the filling of Mr.\n",
      "\n",
      "-\n",
      "Input sentence: “Howd elighted Miss Daryc willh be to receivesuch a letter! ” He madle no ansvwer.\n",
      "Decoded sentence: “No, sir; I don’t know what I have not donging workn’s to me to comply there.\n",
      "\n",
      "-\n",
      "Input sentence: In all hiœs cringing“ attitudes, the God-fugiive is now too plainly known.\n",
      "Decoded sentence: In has mine she the getting of a mas, as when else to me to tell a wain.\n",
      "\n",
      "-\n",
      "Input sentence: “Let us go back t othe cabinet.\n",
      "Decoded sentence: “She will do cerrare in Mrs.\n",
      "\n",
      "-\n",
      "Input sentence: And so would Anne,x i ferh ealth had allowe,d her to apply.\n",
      "Decoded sentence: And they deeceded by the letter from the connection.\n",
      "\n",
      "-\n",
      "Input sentence: Shame upon thme! Pu tone foot upon the table.\n",
      "Decoded sentence: Let me go not an eary of it, sircesh.\n",
      "\n",
      "-\n",
      "Input sentence: It  never wriggles.\n",
      "Decoded sentence: I found there.\n",
      "\n",
      "-\n",
      "Input sentence: ” When tthe gnetlemBen 6had joinedd hem, and tea was over, the cyard-tbals were placedU.\n",
      "Decoded sentence: ” The by horsor was storted the sen found of the word is in like it all.\n",
      "\n",
      "-\n",
      "Input sentence: ”“ Yes, but I should lie to see him.\n",
      "Decoded sentence: ” “You are very gloasedy to Mr.\n",
      "\n",
      "-\n",
      "Input sentence: “He’s a lively chief mat,e that; god man nad a pious; but ll aliv emnow, Im ust trn to.\n",
      "Decoded sentence: “He would difacced to him, stender! ” cried Elizabeth which it no lose to go on.\n",
      "\n",
      "-\n",
      "Input sentence: PIP shrBinking under thew indlas.\n",
      "Decoded sentence: And they all ever your two most cay.\n",
      "\n",
      "-\n",
      "Input sentence: 0But Dhe swiftly calls “awaythe Captai from that sce?nt.\n",
      "Decoded sentence: By had some master, therefore, therefore was father.\n",
      "\n",
      "-\n",
      "Input sentence: ” “Teh person 1of whom Ispeak si agentleman, and a stranger.\n",
      "Decoded sentence: ” “What do you want of the courta in the any none on your descrepe.\n",
      "\n",
      "-\n",
      "Input sentence: Q‘hWat a2 funny watch! ’ sh eremarked.\n",
      "Decoded sentence: ‘Then ye? ’ said the King.\n",
      "\n",
      "-\n",
      "Input sentence: TBeside,s I had a contempt for the uses of odern naural philosophy.\n",
      "Decoded sentence: The shout stick they survess the fiven sumpisedly for her fame.\n",
      "\n",
      "-\n",
      "Input sentence: ’ Itl wEs, no doubt:o Bnly Alice did not like to be told so.\n",
      "Decoded sentence: ’ And she went of many of a much for a molest a twick.\n",
      "\n",
      "-\n",
      "Input sentence: Jenkinson's room.\n",
      "Decoded sentence: Hyde at exactoneed.\n",
      "\n",
      "-\n",
      "Input sentence: ” Y“es, indeed, and received no icnonEsiderablepleasure from the sight.\n",
      "Decoded sentence: ” “You must never heard of nobre than these fauries of the whale.\n",
      "\n",
      "-\n",
      "Input sentence: CHAPTER 12.\n",
      "Decoded sentence: CHAPTER 66.\n",
      "\n",
      "-\n",
      "Input sentence: That is not hids faultthough.\n",
      "Decoded sentence: This was no sen-faction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

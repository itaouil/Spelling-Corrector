{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Useful globals\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find the data\n",
    "base_dir = \"./data\"\n",
    "\n",
    "def load_data(language):\n",
    "    \"\"\"\n",
    "        Loads the books to be processed\n",
    "        and returns them as a list of strings.\n",
    "\n",
    "        Input:\n",
    "            string: Language whose books to load\n",
    "            number: Number of books to process\n",
    "\n",
    "        Ouput:\n",
    "            list: List of strings of the loaded books\n",
    "    \"\"\"\n",
    "    # Output\n",
    "    books = []\n",
    "\n",
    "    # Working path\n",
    "    wrk_path = \"{}/{}\".format(base_dir, language)\n",
    "\n",
    "    # Books' filenames\n",
    "    book_files = os.listdir(wrk_path)\n",
    "\n",
    "    # Read each book and append\n",
    "    for book_file in book_files:\n",
    "        input_file = \"{}/{}\".format(wrk_path, book_file)\n",
    "        with open(input_file) as f:\n",
    "            book = f.read()\n",
    "            books.append(book)\n",
    "\n",
    "    # Useful logs about books\n",
    "    print(\"Loaded {} books for {}.\".format(len(books), language))\n",
    "    for i in range(len(books)):\n",
    "        print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 books for english.\n",
      "There are 74964 words in book2.txt.\n",
      "There are 26436 words in book3.txt.\n",
      "There are 121561 words in book1.txt.\n",
      "There are 211862 words in book4.txt.\n",
      "There are 25572 words in book6.txt.\n"
     ]
    }
   ],
   "source": [
    "books = load_data(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book(book):\n",
    "    \"\"\"\n",
    "        Remove unwanted special characters\n",
    "        and extra spaces from the text.\n",
    "        \n",
    "        Input:\n",
    "            list: List of books\n",
    "        Output:\n",
    "            list: Cleaned list of books\n",
    "    \"\"\"\n",
    "    book = re.sub(r'\\n', ' ', book) \n",
    "    book = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', book)\n",
    "    book = re.sub('a0','', book)\n",
    "    book = re.sub('\\'92t','\\'t', book)\n",
    "    book = re.sub('\\'92s','\\'s', book)\n",
    "    book = re.sub('\\'92m','\\'m', book)\n",
    "    book = re.sub('\\'92ll','\\'ll', book)\n",
    "    book = re.sub('\\'91','', book)\n",
    "    book = re.sub('\\'92','', book)\n",
    "    book = re.sub('\\'93','', book)\n",
    "    book = re.sub('\\'94','', book)\n",
    "    book = re.sub('\\.','. ', book)\n",
    "    book = re.sub('\\!','! ', book)\n",
    "    book = re.sub('\\?','? ', book)\n",
    "    book = re.sub(' +',' ', book)\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean books\n",
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_book(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for vacabolary\n",
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 87 characters.\n",
      "[' ', '!', '$', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'â', 'æ', 'è', 'é', 'ê', 'ô', 'œ', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Check the size of vocabulary and all of the values\n",
    "vocab_size = len(vocab_to_int)\n",
    "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
    "print(sorted(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int to vocab\n",
    "int_to_vocab = {}\n",
    "for character, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18948 sentences\n"
     ]
    }
   ],
   "source": [
    "# Create sentences from the books\n",
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split(\". \"):\n",
    "        sentences.append(sentence + \".\")\n",
    "print(\"There are {} sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert sentences to integers\n",
    "# int_sentences = []\n",
    "# for sentence in sentences:\n",
    "#     int_sentence = []\n",
    "#     for character in sentence:\n",
    "#         int_sentence.append(vocab_to_int[character])\n",
    "#     int_sentences.append(int_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentence's length\n",
    "# lengths = []\n",
    "# for sentence in int_sentences:\n",
    "#     lengths.append(len(sentence))\n",
    "# lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n",
    "# lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 7443 sentences to train and test our model.\n"
     ]
    }
   ],
   "source": [
    "# Limit data for training\n",
    "max_length = 92\n",
    "min_length = 10\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "print(\"We will use {} sentences to train and test our model.\".format(len(good_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise maker (input sequences)\n",
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    \n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_int = np.random.randint(0, len(int_to_vocab)-1)\n",
    "                noisy_sentence.append(int_to_vocab[random_int])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspirited by this wind of promise, my daydreams become more fervent and vivid.\n",
      "InspirHitde by t—his wind of pr-omise, ydaydreams beome more ferevnta nd vivid.\n",
      "\n",
      "This expedition has been the favourite dream of my early years.\n",
      "hTsi epxedition hsa been thef avourie dream of my early years.\n",
      "\n",
      "My education was neglected, yet I was passionately fond of reading.\n",
      "My ducaionL was neglected, yet I was passionateyl fon of reading.\n",
      "\n",
      "You are well acquainted with my failure and how heavily I bore the disappointment.\n",
      "You are well acquainted with my faliure nad hw hevavily TI bore the dsiapponitment.\n",
      "\n",
      "Six years have passed since I resolved on my present undertaking.\n",
      "Six y?ears hae passd since I resovled no my rpesent u:ndertaking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure noise_maker is making mistakes correctly.\n",
    "threshold = 0.9\n",
    "for sentence in good_sentences[:5]:\n",
    "    print(sentence)\n",
    "    print(''.join(noise_maker(sentence, threshold)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and target text\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Shuffle the list of good_senteces\n",
    "np.random.shuffle(good_sentences)\n",
    "\n",
    "# Pad target sentences and build\n",
    "# input and target vocabulary\n",
    "for sentence in good_sentences:\n",
    "    input_text = '\\t' + \"\".join(noise_maker(sentence, 0.9)) + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(sentence)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in sentence:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Set up our encoder and decoder inputs\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# Logging\n",
    "# print('Number of samples:', len(input_texts))\n",
    "# print('Number of unique input tokens:', num_encoder_tokens)\n",
    "# print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# Create dicts for the input and\n",
    "# target vocabolaries\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Crete vectorize data placeholders \n",
    "# for encoder and decoder\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate tensors\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5954 samples, validate on 1489 samples\n",
      "Epoch 1/100\n",
      "5954/5954 [==============================] - 72s 12ms/step - loss: 1.7490 - val_loss: 1.6949\n",
      "Epoch 2/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 1.5537 - val_loss: 1.4940\n",
      "Epoch 3/100\n",
      "5954/5954 [==============================] - 69s 12ms/step - loss: 1.3992 - val_loss: 1.3621\n",
      "Epoch 4/100\n",
      "5954/5954 [==============================] - 69s 12ms/step - loss: 1.3141 - val_loss: 1.3184\n",
      "Epoch 5/100\n",
      "5954/5954 [==============================] - 69s 12ms/step - loss: 1.2541 - val_loss: 1.2455\n",
      "Epoch 6/100\n",
      "5954/5954 [==============================] - 460s 77ms/step - loss: 1.2053 - val_loss: 1.2158\n",
      "Epoch 7/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 1.1646 - val_loss: 1.1815\n",
      "Epoch 8/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 1.1295 - val_loss: 1.1830\n",
      "Epoch 9/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 1.0994 - val_loss: 1.1216\n",
      "Epoch 10/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 1.0786 - val_loss: 1.0894\n",
      "Epoch 11/100\n",
      "5954/5954 [==============================] - 945s 159ms/step - loss: 1.0478 - val_loss: 1.0774\n",
      "Epoch 12/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 1.0235 - val_loss: 1.0400\n",
      "Epoch 13/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 1.0022 - val_loss: 1.0571\n",
      "Epoch 14/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 1.0047 - val_loss: 1.0645\n",
      "Epoch 15/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 0.9741 - val_loss: 1.0190\n",
      "Epoch 16/100\n",
      "5954/5954 [==============================] - 278s 47ms/step - loss: 0.9509 - val_loss: 1.0095\n",
      "Epoch 17/100\n",
      "5954/5954 [==============================] - 716s 120ms/step - loss: 0.9348 - val_loss: 0.9684\n",
      "Epoch 18/100\n",
      "5954/5954 [==============================] - 100s 17ms/step - loss: 0.9185 - val_loss: 0.9556\n",
      "Epoch 19/100\n",
      "5954/5954 [==============================] - 90s 15ms/step - loss: 0.9039 - val_loss: 0.9800\n",
      "Epoch 20/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.8905 - val_loss: 0.9523\n",
      "Epoch 21/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.8747 - val_loss: 0.9192\n",
      "Epoch 22/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.8618 - val_loss: 0.9291\n",
      "Epoch 23/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.8497 - val_loss: 0.9310\n",
      "Epoch 24/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.8377 - val_loss: 0.9197\n",
      "Epoch 25/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.8265 - val_loss: 0.8991\n",
      "Epoch 26/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.8165 - val_loss: 0.9047\n",
      "Epoch 27/100\n",
      "5954/5954 [==============================] - 74s 12ms/step - loss: 0.8059 - val_loss: 0.9039\n",
      "Epoch 28/100\n",
      "5954/5954 [==============================] - 67s 11ms/step - loss: 0.7965 - val_loss: 0.8880\n",
      "Epoch 29/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.7870 - val_loss: 0.8854\n",
      "Epoch 30/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.7775 - val_loss: 0.8906\n",
      "Epoch 31/100\n",
      "5954/5954 [==============================] - 70s 12ms/step - loss: 0.7692 - val_loss: 0.8853\n",
      "Epoch 32/100\n",
      "5954/5954 [==============================] - 79s 13ms/step - loss: 0.7600 - val_loss: 0.8788\n",
      "Epoch 33/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.7516 - val_loss: 0.8755\n",
      "Epoch 34/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.7431 - val_loss: 0.8790\n",
      "Epoch 35/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.7351 - val_loss: 0.8665\n",
      "Epoch 36/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.7265 - val_loss: 0.8880\n",
      "Epoch 37/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.7191 - val_loss: 0.8588\n",
      "Epoch 38/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.7116 - val_loss: 0.8752\n",
      "Epoch 39/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.7035 - val_loss: 0.8718\n",
      "Epoch 40/100\n",
      "5954/5954 [==============================] - 83s 14ms/step - loss: 0.6964 - val_loss: 0.8791\n",
      "Epoch 41/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.6888 - val_loss: 0.8746\n",
      "Epoch 42/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6820 - val_loss: 0.8597\n",
      "Epoch 43/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6750 - val_loss: 0.8847\n",
      "Epoch 44/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6674 - val_loss: 0.8896\n",
      "Epoch 45/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6606 - val_loss: 0.8845\n",
      "Epoch 46/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6535 - val_loss: 0.8714\n",
      "Epoch 47/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6475 - val_loss: 0.8815\n",
      "Epoch 48/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6412 - val_loss: 0.8817\n",
      "Epoch 49/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.6339 - val_loss: 0.8847\n",
      "Epoch 50/100\n",
      "5954/5954 [==============================] - 83s 14ms/step - loss: 0.6274 - val_loss: 0.9145\n",
      "Epoch 51/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6218 - val_loss: 0.8804\n",
      "Epoch 52/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.6154 - val_loss: 0.9111\n",
      "Epoch 53/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.6091 - val_loss: 0.8962\n",
      "Epoch 54/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.6033 - val_loss: 0.8879\n",
      "Epoch 55/100\n",
      "5954/5954 [==============================] - 85s 14ms/step - loss: 0.5967 - val_loss: 0.9260\n",
      "Epoch 56/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.5910 - val_loss: 0.9096\n",
      "Epoch 57/100\n",
      "5954/5954 [==============================] - 73s 12ms/step - loss: 0.5858 - val_loss: 0.9040\n",
      "Epoch 58/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5800 - val_loss: 0.9246\n",
      "Epoch 59/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5744 - val_loss: 0.9124\n",
      "Epoch 60/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5687 - val_loss: 0.9379\n",
      "Epoch 61/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5629 - val_loss: 0.9285\n",
      "Epoch 62/100\n",
      "5954/5954 [==============================] - 67s 11ms/step - loss: 0.5578 - val_loss: 0.9245\n",
      "Epoch 63/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5530 - val_loss: 0.9317\n",
      "Epoch 64/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.5466 - val_loss: 0.9493\n",
      "Epoch 65/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.5424 - val_loss: 0.9447\n",
      "Epoch 66/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.5377 - val_loss: 0.9500\n",
      "Epoch 67/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.5322 - val_loss: 0.9608\n",
      "Epoch 68/100\n",
      "5954/5954 [==============================] - 82s 14ms/step - loss: 0.5275 - val_loss: 0.9768\n",
      "Epoch 69/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.5227 - val_loss: 0.9693\n",
      "Epoch 70/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.5190 - val_loss: 0.9795\n",
      "Epoch 71/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.5140 - val_loss: 0.9913\n",
      "Epoch 72/100\n",
      "5954/5954 [==============================] - 83s 14ms/step - loss: 0.5085 - val_loss: 1.0107\n",
      "Epoch 73/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.5056 - val_loss: 0.9792\n",
      "Epoch 74/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.5006 - val_loss: 0.9959\n",
      "Epoch 75/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4963 - val_loss: 0.9927\n",
      "Epoch 76/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4929 - val_loss: 0.9977\n",
      "Epoch 77/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4882 - val_loss: 1.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4840 - val_loss: 1.0135\n",
      "Epoch 79/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4797 - val_loss: 1.0212\n",
      "Epoch 80/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 0.4762 - val_loss: 1.0287\n",
      "Epoch 81/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 0.4721 - val_loss: 1.0248\n",
      "Epoch 82/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 0.4682 - val_loss: 1.0324\n",
      "Epoch 83/100\n",
      "5954/5954 [==============================] - 79s 13ms/step - loss: 0.4640 - val_loss: 1.0563\n",
      "Epoch 84/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4616 - val_loss: 1.0378\n",
      "Epoch 85/100\n",
      "5954/5954 [==============================] - 79s 13ms/step - loss: 0.4575 - val_loss: 1.0602\n",
      "Epoch 86/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.4543 - val_loss: 1.0695\n",
      "Epoch 87/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.4507 - val_loss: 1.0490\n",
      "Epoch 88/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.4479 - val_loss: 1.0616\n",
      "Epoch 89/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4447 - val_loss: 1.0601\n",
      "Epoch 90/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4413 - val_loss: 1.0681\n",
      "Epoch 91/100\n",
      "5954/5954 [==============================] - 80s 14ms/step - loss: 0.4381 - val_loss: 1.0755\n",
      "Epoch 92/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4354 - val_loss: 1.0932\n",
      "Epoch 93/100\n",
      "5954/5954 [==============================] - 81s 14ms/step - loss: 0.4321 - val_loss: 1.0902\n",
      "Epoch 94/100\n",
      "5954/5954 [==============================] - 71s 12ms/step - loss: 0.4290 - val_loss: 1.0944\n",
      "Epoch 95/100\n",
      "5954/5954 [==============================] - 68s 11ms/step - loss: 0.4264 - val_loss: 1.0737\n",
      "Epoch 96/100\n",
      "5954/5954 [==============================] - 67s 11ms/step - loss: 0.4243 - val_loss: 1.0922\n",
      "Epoch 97/100\n",
      "5954/5954 [==============================] - 67s 11ms/step - loss: 0.4211 - val_loss: 1.1221\n",
      "Epoch 98/100\n",
      "5954/5954 [==============================] - 73s 12ms/step - loss: 0.4184 - val_loss: 1.1168\n",
      "Epoch 99/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4151 - val_loss: 1.1119\n",
      "Epoch 100/100\n",
      "5954/5954 [==============================] - 80s 13ms/step - loss: 0.4133 - val_loss: 1.1187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e776ef0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dailand10/Desktop/dlbaby/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('charSeq2Seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-184-3ca34360fce3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-184-3ca34360fce3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    target_token_index['\\']\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "target_token_index[\"\\\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-e17d64ded27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-d8ba0444ce79>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Populate the first character of target sequence with the start character.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtarget_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_token_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Sampling loop for a batch of sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\t'"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(' ', 0), ('!', 1), ('&', 2), (\"'\", 3), (',', 4), ('-', 5), ('.', 6), ('0', 7), ('1', 8), ('2', 9), ('3', 10), ('4', 11), ('5', 12), ('6', 13), ('7', 14), ('8', 15), ('9', 16), (':', 17), (';', 18), ('?', 19), ('A', 20), ('B', 21), ('C', 22), ('D', 23), ('E', 24), ('F', 25), ('G', 26), ('H', 27), ('I', 28), ('J', 29), ('K', 30), ('L', 31), ('M', 32), ('N', 33), ('O', 34), ('P', 35), ('Q', 36), ('R', 37), ('S', 38), ('T', 39), ('U', 40), ('V', 41), ('W', 42), ('X', 43), ('Y', 44), ('Z', 45), ('`', 46), ('a', 47), ('b', 48), ('c', 49), ('d', 50), ('e', 51), ('f', 52), ('g', 53), ('h', 54), ('i', 55), ('j', 56), ('k', 57), ('l', 58), ('m', 59), ('n', 60), ('o', 61), ('p', 62), ('q', 63), ('r', 64), ('s', 65), ('t', 66), ('u', 67), ('v', 68), ('w', 69), ('x', 70), ('y', 71), ('z', 72), ('æ', 73), ('—', 74), ('‘', 75), ('’', 76), ('“', 77), ('”', 78)])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
